{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f027b4-db19-45a0-a620-f3773f0520bb",
   "metadata": {},
   "source": [
    "# Multi-modal eval: Image resolution\n",
    "\n",
    "`Multi-modal slide decks` is a public dataset that contains a dataset of question-answer pairs from slide decks with visual content.\n",
    "\n",
    "The question-answer pairs are derived from the visual content in the decks, testing the ability of RAG to perform visual reasoning.\n",
    "\n",
    "GPT-4 can be used to answer questions based upon visual slide content, but [image resolution](https://community.openai.com/t/400-errors-on-gpt-vision-api-since-today/534538/16) is a question:\n",
    "\n",
    "* Higher resolution costs more tokens, but also leads to flakiness from the API (BadRequestErrors)\n",
    "* Lower resolution reduces costs and errors, but sacrifices performance\n",
    "\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92071f1-0d9c-4aad-91e1-1abe1ff0b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain langsmith langchain_benchmarks\n",
    "# %pip install -U openai chromadb pypdfium2 open-clip-torch pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc9a30-23fe-4f7c-b479-57442a77290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "env_vars = [\"LANGCHAIN_API_KEY\", \"OPENAI_API_KEY\"]\n",
    "for var in env_vars:\n",
    "    if var not in os.environ:\n",
    "        os.environ[var] = getpass.getpass(prompt=f\"Enter your {var}: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288790a-6e5b-49bc-81b7-cc69788c0a8f",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We can browse the available LangChain benchmark datasets for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f112ad-620f-4d5b-9258-b42925fba339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                   </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LangChain Docs Q&A     </td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Reports</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Multi-modal slide decks</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d\" target=\"_blank\" rel=\"noopener\">40afc8e7-9d7e-44ed-8971-2cae1eb59731</a></td><td>This public dataset is a work-in-progress and will be extended over time.\n",
       "        \n",
       "Questions and answers based on slide decks containing visual tables and charts.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", get_docs=<function load_cached_docs at 0x126b471f0>, retriever_factories={'basic': <function _chroma_retriever_factory at 0x1279c65e0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x1279c6670>, 'hyde': <function _chroma_hyde_retriever_factory at 0x1279c6700>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x126b473a0>}), RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", get_docs=<function load_docs at 0x1279c6c10>, retriever_factories={'basic': <function _chroma_retriever_factory at 0x1279c6ca0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x1279c6d30>, 'hyde': <function _chroma_hyde_retriever_factory at 0x1279c6dc0>}, architecture_factories={}), RetrievalTask(name='Multi-modal slide decks', dataset_id='https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d', description='This public dataset is a work-in-progress and will be extended over time.\\n        \\nQuestions and answers based on slide decks containing visual tables and charts.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\n', get_docs={}, retriever_factories={}, architecture_factories={})])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry\n",
    "\n",
    "registry = registry.filter(Type=\"RetrievalTask\")\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9e3fa0-f96d-44f4-bb85-f6d247a1d685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>Multi-modal slide decks                                                                                                                                    </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID            </td><td><a href=\"https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d\" target=\"_blank\" rel=\"noopener\">40afc8e7-9d7e-44ed-8971-2cae1eb59731</a></td></tr>\n",
       "<tr><td>Description           </td><td>This public dataset is a work-in-progress and will be extended over time.\n",
       "        \n",
       "Questions and answers based on slide decks containing visual tables and charts.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.                                                                                                                                                            </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>                                                                                                                                                           </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>                                                                                                                                                           </td></tr>\n",
       "<tr><td>get_docs              </td><td>{}                                                                                                                                                         </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='Multi-modal slide decks', dataset_id='https://smith.langchain.com/public/40afc8e7-9d7e-44ed-8971-2cae1eb59731/d', description='This public dataset is a work-in-progress and will be extended over time.\\n        \\nQuestions and answers based on slide decks containing visual tables and charts.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\n', get_docs={}, retriever_factories={}, architecture_factories={})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = registry[\"Multi-modal slide decks\"]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2fdb75-9a41-47fe-bd30-4456738f9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Multi-modal slide decks already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(task.dataset_id, dataset_name=task.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8428f102-ffb8-471b-9713-91ed01c6a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_benchmarks.rag.tasks.multi_modal_slide_decks import get_file_names\n",
    "\n",
    "file_names = list(get_file_names())  # PosixPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f88a7a-ac2d-46ff-9fa3-691305a2d589",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "For each presentation, extract an image for each slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9ddb1f-3a40-4615-9885-c82cbf7d82cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 30 images for DDOG_Q3_earnings_deck.pdf\n",
      "Error with image 11: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n",
      "Error with image 5: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n",
      "Error with image 25: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n",
      "Error with image 5: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemini produced an empty response.\n",
      "Unrecognized role: . Treating as a ChatMessage.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import base64\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "import pypdfium2 as pdfium\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import uuid\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "\n",
    "def get_images(file):\n",
    "    \"\"\"\n",
    "    Get PIL images from PDF pages and save them to a specified directory\n",
    "    :param file: Path to file\n",
    "    :return: A list of PIL images\n",
    "    \"\"\"\n",
    "\n",
    "    # Get presentation\n",
    "    pdf = pdfium.PdfDocument(file)\n",
    "    n_pages = len(pdf)\n",
    "\n",
    "    # Get images\n",
    "    pil_images = []\n",
    "    print(f\"Extracting {n_pages} images for {file.name}\")\n",
    "    for page_number in range(n_pages):\n",
    "        page = pdf.get_page(page_number)\n",
    "        bitmap = page.render(scale=1, rotation=0, crop=(0, 0, 0, 0))\n",
    "        pil_image = bitmap.to_pil()\n",
    "        pil_images.append(pil_image)\n",
    "    return pil_images\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "\n",
    "    :param base64_string: Base64 string\n",
    "    :param size: Image size\n",
    "    :return: Re-sized Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def convert_to_base64(pil_image, size):\n",
    "    \"\"\"\n",
    "    Convert PIL images to Base64 encoded strings\n",
    "\n",
    "    :param pil_image: PIL image\n",
    "    :param size: Tuple w/ img size\n",
    "    :return: Re-sized Base64 string\n",
    "    \"\"\"\n",
    "\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"JPEG\")  # You can change the format if needed\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    img_str = resize_base64_image(img_str, size=size)\n",
    "    return img_str\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt, llm):\n",
    "    \"\"\"\n",
    "    Make image summary\n",
    "\n",
    "    :param img_base64: Base64 encoded string for image\n",
    "    :param prompt: Text prompt for summarizatiomn\n",
    "    :return: Image summarization prompt\n",
    "\n",
    "    \"\"\"\n",
    "    if llm == \"gpt4v\":\n",
    "        chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "    elif llm == \"gemini\":\n",
    "        chat = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(img_base64_list, llm):\n",
    "    \"\"\"\n",
    "    Generate summaries for images\n",
    "\n",
    "    :param img_base64_list: Base64 encoded images\n",
    "    :param llm: LLM\n",
    "    :return: List of image summaries and processed images\n",
    "    \"\"\"\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "    processed_images = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply summarization to images\n",
    "    for i, base64_image in enumerate(img_base64_list):\n",
    "        try:\n",
    "            image_summaries.append(image_summarize(base64_image, prompt, llm))\n",
    "            processed_images.append(base64_image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with image {i+1}: {e}\")\n",
    "\n",
    "    return image_summaries, processed_images\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(vectorstore, image_summaries, images):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "\n",
    "    :param vectorstore: Vectorstore to store embedded image sumamries\n",
    "    :param image_summaries: Image summaries\n",
    "    :param images: Base64 encoded images\n",
    "    :return: Retriever\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def prepare_images(docs):\n",
    "    \"\"\"\n",
    "    Prepare iamges for prompt\n",
    "\n",
    "    :param docs: A list of base64-encoded images from retriever.\n",
    "    :return: Dict containing a list of base64-encoded strings.\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    for doc in docs:\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        b64_images.append(doc)\n",
    "    return {\"images\": b64_images}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict, num_images=2):\n",
    "    \"\"\"\n",
    "    GPT-4V prompt for image analysis.\n",
    "\n",
    "    :param data_dict: A dict with images and a user-provided question.\n",
    "    :param num_images: Number of images to include in the prompt.\n",
    "    :return: A list containing message objects for each image and the text prompt.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"][:num_images]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an analyst tasked with answering questions about visual content.\\n\"\n",
    "            \"You will be give a set of image(s) from a slide deck / presentation.\\n\"\n",
    "            \"Use this information to answer the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever, llm):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    :param retriever: Retriever\n",
    "    :param llm: LLM\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    if llm == \"gpt4v\":\n",
    "        model = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "    elif llm == \"gemini\":\n",
    "        model = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(prepare_images),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "\n",
    "# Images\n",
    "images = []\n",
    "for fi in file_names:\n",
    "    images.extend(get_images(fi))\n",
    "\n",
    "# Experiment configurations\n",
    "experiments = [\n",
    "    ((480, 270), \"gpt4v-480-270\", \"gpt4v\"),\n",
    "    ((720, 405), \"gpt4v-720-405\", \"gpt4v\"),\n",
    "    ((960, 540), \"gpt4v-960-540\", \"gpt4v\"),\n",
    "    ((480, 270), \"gemini-480-270\", \"gemini\"),\n",
    "    ((720, 405), \"gemini-720-405\", \"gemini\"),\n",
    "    ((960, 540), \"gemini-960-540\", \"gemini\"),\n",
    "]\n",
    "\n",
    "stor_chain = {}\n",
    "for img_resolution, expt, llm in experiments:\n",
    "    # Base64 strings\n",
    "    images_base_64 = [convert_to_base64(i, img_resolution) for i in images]\n",
    "\n",
    "    # Image summaries\n",
    "    image_summaries, images_base_64_processed = generate_img_summaries(\n",
    "        images_base_64, llm\n",
    "    )\n",
    "\n",
    "    # Vectorstore o index the summaries\n",
    "    vectorstore = Chroma(collection_name=expt, embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "    # Create retriever\n",
    "    retriever = create_multi_vector_retriever(\n",
    "        vectorstore,\n",
    "        image_summaries,\n",
    "        images_base_64_processed,\n",
    "    )\n",
    "\n",
    "    stor_chain[expt] = multi_modal_rag_chain(retriever, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32590a7b-8fd1-46fd-a34b-9d3207cc7b0a",
   "metadata": {},
   "source": [
    "## Eval\n",
    "\n",
    "Run evaluation on our dataset:\n",
    "\n",
    "* `task.name` is the dataset of QA pairs that we cloned\n",
    "* `eval_config` specifies the [LangSmith evaluator](https://docs.smith.langchain.com/evaluation/evaluator-implementations#correctness-qa-evaluation) for our dataset, which will use GPT-4 as a grader\n",
    "* The grader will evaluate the chain-generated answer to each question relative to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939b5273-8633-41c1-9d8d-e455b1584a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'gpt4v-480-270-ceff' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74/compare?selectedSessions=0e53beb9-68da-4cf7-86b0-c9277f53246e\n",
      "\n",
      "View all tests for Dataset Multi-modal slide decks at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74\n",
      "[------------------------------------------------->] 10/10"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>As of the latest information provided in the i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>972df170-09dc-4900-9d76-b197d0a4e3b5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.055602</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.083101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.434943</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.127925</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.379362</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.739249</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.110107</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top     As of the latest information provided in the i...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "        feedback.COT Contextual Accuracy error  execution_time  \\\n",
       "count                          10.000000     0       10.000000   \n",
       "unique                               NaN     0             NaN   \n",
       "top                                  NaN   NaN             NaN   \n",
       "freq                                 NaN   NaN             NaN   \n",
       "mean                            0.500000   NaN       10.055602   \n",
       "std                             0.527046   NaN        3.083101   \n",
       "min                             0.000000   NaN        6.434943   \n",
       "25%                             0.000000   NaN        7.127925   \n",
       "50%                             0.500000   NaN       10.379362   \n",
       "75%                             1.000000   NaN       12.739249   \n",
       "max                             1.000000   NaN       14.110107   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     10  \n",
       "unique                                    10  \n",
       "top     972df170-09dc-4900-9d76-b197d0a4e3b5  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'gpt4v-720-405-ceff' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74/compare?selectedSessions=5b810cb3-25f9-45c1-91f6-8807e44e1e42\n",
      "\n",
      "View all tests for Dataset Multi-modal slide decks at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74\n",
      "[>                                                 ] 0/10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 4529633d-b9a8-4565-83d6-f65b1ff9dd80 with inputs {'Question': 'What is the projected TAM for observability expected for each year through 2026?'}\n",
      "Error Type: BadRequestError, Message: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---->                                             ] 1/10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 16d929e5-f2ef-43ea-a408-9192b00bd8e2 with inputs {'Question': \"What is the projected cloud spend in $B's in 2026E?\"}\n",
      "Error Type: BadRequestError, Message: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------->          ] 8/10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example b682bce8-bc6d-4d87-95aa-e56546a57a33 with inputs {'Question': 'What was the % Y/Y growth in FY20, FY21, and FY22?'}\n",
      "Error Type: BadRequestError, Message: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 10/10"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>As of the data provided in the slide, Datadog ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Error code: 400 - {'error': {'message': \"You u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e9ea6a99-77fd-4a53-9c11-a46c5463daf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.191968</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.482067</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.256001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.690568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.492313</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.067811</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.789134</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "count                                                   7   \n",
       "unique                                                  7   \n",
       "top     As of the data provided in the slide, Datadog ...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "        feedback.COT Contextual Accuracy  \\\n",
       "count                                7.0   \n",
       "unique                               NaN   \n",
       "top                                  NaN   \n",
       "freq                                 NaN   \n",
       "mean                                 1.0   \n",
       "std                                  0.0   \n",
       "min                                  1.0   \n",
       "25%                                  1.0   \n",
       "50%                                  1.0   \n",
       "75%                                  1.0   \n",
       "max                                  1.0   \n",
       "\n",
       "                                                    error  execution_time  \\\n",
       "count                                                   3       10.000000   \n",
       "unique                                                  3             NaN   \n",
       "top     Error code: 400 - {'error': {'message': \"You u...             NaN   \n",
       "freq                                                    1             NaN   \n",
       "mean                                                  NaN       10.191968   \n",
       "std                                                   NaN        2.482067   \n",
       "min                                                   NaN        7.256001   \n",
       "25%                                                   NaN        8.690568   \n",
       "50%                                                   NaN        9.492313   \n",
       "75%                                                   NaN       11.067811   \n",
       "max                                                   NaN       15.789134   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     10  \n",
       "unique                                    10  \n",
       "top     e9ea6a99-77fd-4a53-9c11-a46c5463daf4  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'gpt4v-960-540-ceff' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74/compare?selectedSessions=65ede05e-4460-43cd-b9ba-01ad52228a35\n",
      "\n",
      "View all tests for Dataset Multi-modal slide decks at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74\n",
      "[>                                                 ] 0/10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 16d929e5-f2ef-43ea-a408-9192b00bd8e2 with inputs {'Question': \"What is the projected cloud spend in $B's in 2026E?\"}\n",
      "Error Type: BadRequestError, Message: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---->                                             ] 1/10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 4529633d-b9a8-4565-83d6-f65b1ff9dd80 with inputs {'Question': 'What is the projected TAM for observability expected for each year through 2026?'}\n",
      "Error Type: BadRequestError, Message: Error code: 400 - {'error': {'message': \"You uploaded an unsupported image. Please make sure your image is below 20 MB in size and is of one the following formats: ['png', 'jpeg', 'gif', 'webp'].\", 'type': 'invalid_request_error', 'param': None, 'code': 'image_parse_error'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 10/10"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Based on the second image provided, Datadog ha...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Error code: 400 - {'error': {'message': \"You u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2f83f649-41c4-4f59-99d5-2b3a63fcbff0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.496728</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.784272</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.608510</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.929827</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.544359</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.262577</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.469452</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "count                                                   8   \n",
       "unique                                                  8   \n",
       "top     Based on the second image provided, Datadog ha...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "        feedback.COT Contextual Accuracy  \\\n",
       "count                                8.0   \n",
       "unique                               NaN   \n",
       "top                                  NaN   \n",
       "freq                                 NaN   \n",
       "mean                                 1.0   \n",
       "std                                  0.0   \n",
       "min                                  1.0   \n",
       "25%                                  1.0   \n",
       "50%                                  1.0   \n",
       "75%                                  1.0   \n",
       "max                                  1.0   \n",
       "\n",
       "                                                    error  execution_time  \\\n",
       "count                                                   2       10.000000   \n",
       "unique                                                  2             NaN   \n",
       "top     Error code: 400 - {'error': {'message': \"You u...             NaN   \n",
       "freq                                                    1             NaN   \n",
       "mean                                                  NaN       12.496728   \n",
       "std                                                   NaN        1.784272   \n",
       "min                                                   NaN        8.608510   \n",
       "25%                                                   NaN       11.929827   \n",
       "50%                                                   NaN       12.544359   \n",
       "75%                                                   NaN       13.262577   \n",
       "max                                                   NaN       15.469452   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     10  \n",
       "unique                                    10  \n",
       "top     2f83f649-41c4-4f59-99d5-2b3a63fcbff0  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'gemini-480-270-ceff' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74/compare?selectedSessions=6e116420-5e6b-4a1a-9e2d-11b198556899\n",
      "\n",
      "View all tests for Dataset Multi-modal slide decks at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74\n",
      "[------------------------------------------------->] 10/10"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>As of September 30, 2022, Datadog had 26,800 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>989b94d0-bddd-439a-ae93-594baab68263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.221055</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.643574</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.430064</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.744385</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.118048</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.740429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.230219</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top      As of September 30, 2022, Datadog had 26,800 ...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "        feedback.COT Contextual Accuracy error  execution_time  \\\n",
       "count                          10.000000     0       10.000000   \n",
       "unique                               NaN     0             NaN   \n",
       "top                                  NaN   NaN             NaN   \n",
       "freq                                 NaN   NaN             NaN   \n",
       "mean                            0.600000   NaN       10.221055   \n",
       "std                             0.516398   NaN        0.643574   \n",
       "min                             0.000000   NaN        9.430064   \n",
       "25%                             0.000000   NaN        9.744385   \n",
       "50%                             1.000000   NaN       10.118048   \n",
       "75%                             1.000000   NaN       10.740429   \n",
       "max                             1.000000   NaN       11.230219   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     10  \n",
       "unique                                    10  \n",
       "top     989b94d0-bddd-439a-ae93-594baab68263  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'gemini-720-405-ceff' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74/compare?selectedSessions=8fef994a-fe86-4252-b834-6032b0222a6f\n",
      "\n",
      "View all tests for Dataset Multi-modal slide decks at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74\n",
      "[------------------------------------------------->] 10/10"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>As of September 30, 2022, Datadog had 26,800 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1e3158d0-d8cc-4859-86f9-4765fd4d23ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.710439</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.927829</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.496370</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.256355</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.361182</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.902680</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.507018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top      As of September 30, 2022, Datadog had 26,800 ...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "        feedback.COT Contextual Accuracy error  execution_time  \\\n",
       "count                          10.000000     0       10.000000   \n",
       "unique                               NaN     0             NaN   \n",
       "top                                  NaN   NaN             NaN   \n",
       "freq                                 NaN   NaN             NaN   \n",
       "mean                            0.900000   NaN       12.710439   \n",
       "std                             0.316228   NaN        0.927829   \n",
       "min                             0.000000   NaN       11.496370   \n",
       "25%                             1.000000   NaN       12.256355   \n",
       "50%                             1.000000   NaN       12.361182   \n",
       "75%                             1.000000   NaN       12.902680   \n",
       "max                             1.000000   NaN       14.507018   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     10  \n",
       "unique                                    10  \n",
       "top     1e3158d0-d8cc-4859-86f9-4765fd4d23ab  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'gemini-960-540-ceff' at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74/compare?selectedSessions=aabcb791-febd-4a8e-9f80-9db2ab5a0960\n",
      "\n",
      "View all tests for Dataset Multi-modal slide decks at:\n",
      "https://smith.langchain.com/o/1fa8b1f4-fcb9-4072-9aa9-983e35ad61b8/datasets/cd8e425b-5769-4b5e-a784-cbf8e2d72c74\n",
      "[------------------------------------------------->] 10/10"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>feedback.COT Contextual Accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>As of September 30, 2022, Datadog had approxi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eebfb034-ca79-4128-90a4-ec8e93d50521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.657744</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.527046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.688655</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.549547</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.640496</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.683292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.265661</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.560468</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   output  \\\n",
       "count                                                  10   \n",
       "unique                                                 10   \n",
       "top      As of September 30, 2022, Datadog had approxi...   \n",
       "freq                                                    1   \n",
       "mean                                                  NaN   \n",
       "std                                                   NaN   \n",
       "min                                                   NaN   \n",
       "25%                                                   NaN   \n",
       "50%                                                   NaN   \n",
       "75%                                                   NaN   \n",
       "max                                                   NaN   \n",
       "\n",
       "        feedback.COT Contextual Accuracy error  execution_time  \\\n",
       "count                          10.000000     0       10.000000   \n",
       "unique                               NaN     0             NaN   \n",
       "top                                  NaN   NaN             NaN   \n",
       "freq                                 NaN   NaN             NaN   \n",
       "mean                            0.500000   NaN       15.657744   \n",
       "std                             0.527046   NaN        1.688655   \n",
       "min                             0.000000   NaN       13.549547   \n",
       "25%                             0.000000   NaN       14.640496   \n",
       "50%                             0.500000   NaN       15.683292   \n",
       "75%                             1.000000   NaN       16.265661   \n",
       "max                             1.000000   NaN       19.560468   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     10  \n",
       "unique                                    10  \n",
       "top     eebfb034-ca79-4128-90a4-ec8e93d50521  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.smith import RunEvalConfig\n",
    "from langsmith.client import Client\n",
    "\n",
    "# Evaluator configuration\n",
    "client = Client()\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    ")\n",
    "\n",
    "# Experiments\n",
    "chain_map = {\n",
    "    \"gpt4v-480-270\": stor_chain[\"gpt4v-480-270\"],\n",
    "    \"gpt4v-720-405\": stor_chain[\"gpt4v-720-405\"],\n",
    "    \"gpt4v-960-540\": stor_chain[\"gpt4v-960-540\"],\n",
    "    \"gemini-480-270\": stor_chain[\"gemini-480-270\"],\n",
    "    \"gemini-720-405\": stor_chain[\"gemini-720-405\"],\n",
    "    \"gemini-960-540\": stor_chain[\"gemini-960-540\"],\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "run_id = uuid.uuid4().hex[:4]\n",
    "test_runs = {}\n",
    "for project_name, chain in chain_map.items():\n",
    "    test_runs[project_name] = client.run_on_dataset(\n",
    "        dataset_name=task.name,\n",
    "        llm_or_chain_factory=lambda: (lambda x: x[\"Question\"]) | chain,\n",
    "        evaluation=eval_config,\n",
    "        verbose=True,\n",
    "        project_name=f\"{project_name}-{run_id}\",\n",
    "        project_metadata={\"chain\": project_name},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
