{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bb467d-861d-4b07-a48d-8e5aa177c969",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Let's evaluate your architecture on a Q&A dataset for the LangChain python docs.\n",
    "\n",
    "Common RAG architectures have two main components:\n",
    "1. Retriever -> provides information from a knowledge base. Vector search is simple and powerful, but this can include any database or arbitrary search engine\n",
    "2. Response generator -> synthesizes a response to the user input based on a mixture of learned knowledge and the retrieved input.\n",
    "\n",
    "Focusing on retrievers for unstructured data: you still have some additional design decisions you may want to make:\n",
    "\n",
    "- What chunk size(s) to use for each document: too large and your system will be able to consider fewer documents at a time. Too small and the chunks themselves lack important context needed to interpret their content.\n",
    "- How to index a single chunk: generating a single vector from an embedding model may be fine, or you can generate additional vectors based on summaries, hypothetical questions, or other related content. Some may even consider incorporating a keyword index or other structured metadata to better support different types of searches.\n",
    "- How to assemble the retrieved chunks: once you've fetched the k-best list of \"relevant\" documents, you may want to do things like:\n",
    "  - re-integrate the document into its parent context.\n",
    "  - rerank the documents based on other criteria\n",
    " \n",
    "All of these options come with tradeoffs in cost, response quality, and time. This may seem overwhelming at first! The good news is that the retrieval and response mechanism can be modular -> the better the information, the better the response, and the better the LLM, the better it is able to integrate the knowledge.\n",
    "\n",
    "This notebook provides a RAG gym/playground you can use to evaluate different RAG strategies on a Q&A dataset generated from LangChain's python docs. The intent is to make it easy to experiment with different techniques to see their tradeoffs and make the appropriate decision for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49db759-7ce6-4ab7-a58f-7fc3a6a7c8ec",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "We will install quite a few prerequisites for this example since we are comparing various techinques and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f44b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U langchain_benchmarks\n",
    "# %pip install -U langchain langsmith langchainhub chromadb openai huggingface pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334f96b5-80ca-4b4c-80cf-60d7acb3cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -e ../../../../langchain/libs/langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae13f6-cd40-41e6-bd02-bd683e91cbff",
   "metadata": {},
   "source": [
    "For this code to work, please configure LangSmith environment variables with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0f5e4a-1d3c-4a0f-9ae6-fcdb6638f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LANGCHAIN_ENDPOINT=http://localhost:1984\n"
     ]
    }
   ],
   "source": [
    "%env LANGCHAIN_ENDPOINT=http://localhost:1984\n",
    "%env LANGCHAIN_TRACING_V2=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53fe8fab-ce3c-4d52-bdf7-ff5d0400f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39159d0-9ea1-414f-a9d8-4a7b22b3d2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_benchmarks import clone_public_dataset\n",
    "from langchain_benchmarks.rag import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3644d211-382e-41aa-b282-21b01d28fc35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  ID</th><th>Name              </th><th>Dataset ID                          </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">   0</td><td>LangChain Docs Q&amp;A</td><td>452ccafc-18e1-4314-885b-edd735f17b9d</td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model&#x27;s response relative to the retrieved documents (if any).              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(environments=[RetrievalEnvironment(id=0, name='LangChain Docs Q&A', dataset_id='452ccafc-18e1-4314-885b-edd735f17b9d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x2917389a0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x291738a40>, 'hyde': <function _chroma_hyde_retriever_factory at 0x291738ae0>}, architecture_factories={'chat-langchain': <function default_response_chain at 0x2909b45e0>})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671282f8-c455-4390-b018-e53bbd833093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langchain_docs = registry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6dca5d-63cf-4315-8206-726abe816473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>ID         </td><td>0                                   </td></tr>\n",
       "<tr><td>Name       </td><td>LangChain Docs Q&amp;A                  </td></tr>\n",
       "<tr><td>Dataset ID </td><td>452ccafc-18e1-4314-885b-edd735f17b9d</td></tr>\n",
       "<tr><td>Description</td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides th...                                     </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalEnvironment(id=0, name='LangChain Docs Q&A', dataset_id='452ccafc-18e1-4314-885b-edd735f17b9d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x2917389a0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x291738a40>, 'hyde': <function _chroma_hyde_retriever_factory at 0x291738ae0>}, architecture_factories={'chat-langchain': <function default_response_chain at 0x2909b45e0>})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70369f67-deb4-467a-801a-6d38c3d0460d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset LangChain Docs Q&A already exists. Skipping.\n",
      "You can access the dataset at http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/1e4bf58b-1a61-44fb-bb84-4c5c0e2b4b5b.\n"
     ]
    }
   ],
   "source": [
    "clone_public_dataset(langchain_docs.dataset_id, dataset_name=langchain_docs.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58247f5-b9bd-4cc5-9632-78bc21bb10b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS, Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b6b5205-7464-4bb4-81ce-0079a7646d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f04301-4f13-4dee-8ac8-f9fe88c31add",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_factory = langchain_docs.retriever_factories[\"basic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f83585f-ce88-4084-b7c1-24ec51ff5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes the documents with the specified embeddings\n",
    "# Note that this does not apply any chunking to the docs,\n",
    "# which means the\n",
    "retriever = retriever_factory(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d2e139-2653-4f7b-944b-91ef52f43d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory for creating a chain.\n",
    "chain_factory = langchain_docs.architecture_factories[\"chat-langchain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f9be718-64f0-4706-9527-240a1cdb3ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LCEL stands for LangChain Expression Language. It is a declarative language that allows you to easily compose chains together in LangChain. LCEL was designed to support putting prototypes into production with no code changes. Here are some key features of LCEL:\\n\\n- Streaming support: Chains built with LCEL can stream tokens straight from an LLM to a streaming output parser, providing incremental chunks of output.\\n- Async support: LCEL chains can be called with both synchronous and asynchronous APIs, enabling the same code to be used for prototypes and in production.\\n- Optimized parallel execution: LCEL automatically executes steps in parallel when possible, reducing latency.\\n- Retries and fallbacks: Configurable retries and fallbacks improve the reliability of LCEL chains.\\n- Access intermediate results: LCEL allows you to access intermediate results of complex chains, which can be useful for end-users or debugging.\\n- Input and output schemas: LCEL chains have Pydantic and JSONSchema schemas inferred from the chain structure for input and output validation.\\n- Seamless integration with LangSmith tracing: All steps in LCEL chains are automatically logged to LangSmith for observability and debuggability.\\n- Seamless integration with LangServe deployment: LCEL chains can be easily deployed using LangServe. [0]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "chain_factory(retriever).invoke({\"question\": \"what's lcel?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcf8a8fe-f759-4ddc-a156-0ebb7b547589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with Code Llama on Ollama\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "ollama = ChatOllama(model=\"codellama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8826d4d9-0371-4bfa-b2c0-c886d496b366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLCEL stands for LangChain Expression Language. It's a markup language used to describe natural language conversations using chains of Runnables (the core LCEL interface). The goal is to make it easy to chain together different Runnable components in a sequence that accomplishes a specific task or generates a response. \\n\\nRunnables are the building blocks of LCEL, and there are many built-in ones. Chains of Runnables can be used to do everything from generating text to executing code. \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_factory(retriever, llm=ollama).invoke({\"question\": \"what's lcel?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a64f76-65ae-4367-b43f-f2be3431e7af",
   "metadata": {},
   "source": [
    "Let's test that our agent works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821e4b0-8e67-418a-840c-470fcde42df0",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Let's evaluate a retriever now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513042fe-2878-44f8-ae84-05b9d521c1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain_benchmarks.rag import RAG_EVALUATION\n",
    "from langsmith.client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bedd9d1-fc06-4066-9f89-b874ae818d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aab7514e-a6ef-4c21-b90f-d9cbefcf5af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----->                                            ] 10/86"
     ]
    }
   ],
   "source": [
    "test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, retriever),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86578d5-be5c-4bcd-9dcb-35280eeed3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee992f87-4137-49b1-a1f1-0cc7be0e32d8",
   "metadata": {},
   "source": [
    "# Comparing with other indexing strategies\n",
    "\n",
    "## Parent Document Retriever\n",
    "\n",
    "Let's try on a parent document retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1398f5e3-b7fe-4693-bcc0-c6c6f75c8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_factory = langchain_docs.retriever_factories[\"parent-doc\"]\n",
    "\n",
    "# Indexes the documents with the specified embeddings\n",
    "retriever = retriever_factory(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b1f4b5d-143a-44ce-95f4-d0b5782ada74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-long-leg-61' at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/projects/p/dd556cd9-a156-4e5f-96bb-1c0e480fa230?eval=true\n",
      "\n",
      "View all tests for Dataset LangChain Docs Q&A at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/1e4bf58b-1a61-44fb-bb84-4c5c0e2b4b5b\n",
      "[->                                                ] 4/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 789d3416-cdaf-41be-b3c7-9da71513aeeb with inputs {'question': 'Will this work?\\n\\n```\\nfrom langchain.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI(model=\"claude-2\")\\nllm.predict(\"Hi\")\\n```'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 17895 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------->                                          ] 14/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 0f0322dd-a92f-4042-afd3-4782a9dc9c2f with inputs {'question': 'how to run a runnable'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 56235 tokens. Please reduce the length of the messages.\n",
      "Chain failed for example a04d94f6-f12d-48e9-9128-0b2894c768e5 with inputs {'question': 'what does runnable.predict() mean?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 31952 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------->                                        ] 17/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example a88b6a8d-fe1f-4402-818a-668d7481e04e with inputs {'question': 'What class type is returned by initialize_agent'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 48201 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------->                                        ] 18/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 1f97ba11-f475-4974-b179-3be47f5882ec with inputs {'question': 'How do i run llama 2 in langchain'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 21917 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------>                                     ] 22/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 1a957869-5979-400c-805f-bc089782cb40 with inputs {'question': 'Let\\'s say I have a chain like:\\n\\nmodel_call_1 = (\\n  RunnablePassthrough()\\n  | prompt\\n  | model_parser\\n)\\n# ...\\nchain = model_call_1 #| { \"attr\": model_call_2 } | model_call_3\\nHow can I print out the filled out prompts for each model call?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 27114 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------>                                     ] 23/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 39e0baee-6514-4c18-be15-00364c95bead with inputs {'question': 'soooo, is it possible to pass any kwargs for similarity thresholds or K-top documents in the multivectorretriever as we do in normal retrievers?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 26092 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------->                                   ] 26/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 9b046e8c-4541-4205-a06c-adc19fce2c46 with inputs {'question': 'what does runnable mean'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 48701 tokens. Please reduce the length of the messages.\n",
      "Chain failed for example 023d8950-a200-4747-9d5a-3c63bb61f7eb with inputs {'question': 'what method should subclasses override if they can start producing output while input is still being generated'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 24838 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------->                                 ] 29/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example e0ad7e00-3243-493f-a59f-dc105bb99d96 with inputs {'question': 'How can I use OpenAI functions to get structured outputs in a chain?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 37297 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------->                                 ] 30/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 9d712d95-e9f6-4769-b86a-c773ef852321 with inputs {'question': 'how can I create a vectorstore from the texts in the list'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 17836 tokens. Please reduce the length of the messages.\n",
      "Chain failed for example b4cb6d14-ff16-45d7-9458-6e605a87165a with inputs {'question': 'what does this do? return RunnableBranch(\\n        (\\n            RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\\n                run_name=\"HasChatHistoryCheck\"\\n            ),\\n            conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\\n        ),\\n        (\\n            RunnableLambda(itemgetter(\"question\")).with_config(\\n                run_name=\"Itemgetter:question\"\\n            )\\n            | retriever\\n        ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\\n    ).with_config(run_name=\"RouteDependingOnChatHistory\")'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 24934 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------>                               ] 33/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example a6dd2f88-6ec2-4c6c-bf1e-81e6907f43f2 with inputs {'question': 'how do I search and filter metadata in redis vectorstore?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 19817 tokens. Please reduce the length of the messages.\n",
      "Chain failed for example 31e43b7c-a552-4d64-a3c6-086af9534405 with inputs {'question': 'how do I control the maximum number requests that can be made at the same time when making batch calls?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 34360 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------->                            ] 38/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example f6829b0f-e90d-400e-968e-726dd5ee66a9 with inputs {'question': 'whats the difference between run_id and example_id'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 24044 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------->                           ] 39/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example f0cc53b4-f427-4f82-8b89-85f29f21fefd with inputs {'question': 'which document laaoder should i use for a loading a single web apage?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 59012 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------->                           ] 40/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example b815e7d6-9547-4e93-83c2-84fdb3c02e97 with inputs {'question': 'What is a chain?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 31729 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------------->                          ] 41/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example bd080ab4-780b-4e85-9835-ee8cab4fa2d9 with inputs {'question': 'What are some ways of doing retrieval augmented generation?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 22376 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------>                         ] 43/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 301af9d5-8e5c-4ec4-9cf5-25a88703173a with inputs {'question': 'how many llm api calls are made in OpenAIFunctionsAgent'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 45603 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------->                       ] 47/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 14530d34-51f5-419c-bf62-67f6e1333278 with inputs {'question': \"What's function calling\"}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 27732 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------->                      ] 49/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 7dfa162f-3c38-4c2d-b771-323d169d5312 with inputs {'question': 'im getting a rate limit error for my llm - how do I have it automatically go to a different model?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 18269 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------>                   ] 54/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 8522ca3e-b5c1-4634-8b7f-d9e2b78961dd with inputs {'question': 'How do I use Qdrant as a vector store in the conversational retrieval chain?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 17075 tokens. Please reduce the length of the messages.\n",
      "Chain failed for example 2f67cf5e-9855-4357-a980-19b16bfb0b12 with inputs {'question': 'my agent keeps getting an OutputParserException is something i can set to make it take care of these?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 47406 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------->                ] 59/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 70063d0e-3bc3-4b3e-bd81-d24f7c07b186 with inputs {'question': 'I want to return the source documents of my Weaviate retriever. Show me how'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 39044 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------>             ] 64/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 26d170f9-c0b2-4ccb-a11c-480416041cea with inputs {'question': 'What is html2texttransformer? Does it omit urls?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 48291 tokens. Please reduce the length of the messages.\n",
      "Chain failed for example 674c0c0e-92b5-4c48-91cf-14768624b0bd with inputs {'question': 'what are the main methods supported by Runnables'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 19708 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------------->           ] 67/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example b1e2b78a-0d96-4354-a4a5-c580ad91daef with inputs {'question': 'I want to save the configuration for a given LLM. Show me how to do that.'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 55564 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------------------->         ] 71/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 2440f23d-053d-4bba-b3c4-c1b90d376bb9 with inputs {'question': 'What serialization format is used to serialize chains to and from disk?'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 21008 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------->    ] 79/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example d2c50cf2-a942-4cc1-852a-d7bd449ec4fe with inputs {'question': \"What's an LLMChain\"}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 51937 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------------------------->   ] 81/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 47dfa9ba-7a29-48a8-b5c2-2017bd24ac6f with inputs {'question': \"What's a runnable lambda?\"}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 49897 tokens. Please reduce the length of the messages.\n",
      "Chain failed for example 07453ed9-93ca-4e29-a494-2b145b774dc5 with inputs {'question': 'How do I deal with openai rate limiting by having a backup model? Show me with code.'}\n",
      "Error Type: InvalidRequestError, Message: This model's maximum context length is 16385 tokens. However, your messages resulted in 20758 tokens. Please reduce the length of the messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 86/86\n",
      " Eval quantiles:\n",
      "        embedding_cosine_distance  faithfulness  score_string:accuracy  \\\n",
      "count                   55.000000     18.000000              55.000000   \n",
      "unique                        NaN           NaN                    NaN   \n",
      "top                           NaN           NaN                    NaN   \n",
      "freq                          NaN           NaN                    NaN   \n",
      "mean                     0.109461      0.861111               0.589091   \n",
      "std                      0.068414      0.259272               0.330921   \n",
      "min                      0.031590      0.100000               0.100000   \n",
      "25%                      0.056286      0.775000               0.300000   \n",
      "50%                      0.090218      1.000000               0.700000   \n",
      "75%                      0.140353      1.000000               1.000000   \n",
      "max                      0.278385      1.000000               1.000000   \n",
      "\n",
      "                                                    error  execution_time  \n",
      "count                                                  31       86.000000  \n",
      "unique                                                 31             NaN  \n",
      "top     This model's maximum context length is 16385 t...             NaN  \n",
      "freq                                                    1             NaN  \n",
      "mean                                                  NaN        7.540131  \n",
      "std                                                   NaN        3.710658  \n",
      "min                                                   NaN        2.353036  \n",
      "25%                                                   NaN        5.116402  \n",
      "50%                                                   NaN        6.406112  \n",
      "75%                                                   NaN        8.658157  \n",
      "max                                                   NaN       21.762401  \n"
     ]
    }
   ],
   "source": [
    "parent_doc_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, retriever),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cef0410-47ec-4830-9b75-621eb85240ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_cosine_distance</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>score_string:accuracy</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This model's maximum context length is 16385 t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.109461</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.589091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.540131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.068414</td>\n",
       "      <td>0.259272</td>\n",
       "      <td>0.330921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.710658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.031590</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.353036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.056286</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.116402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.090218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.406112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.140353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.658157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.278385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.762401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        embedding_cosine_distance  faithfulness  score_string:accuracy  \\\n",
       "count                   55.000000     18.000000              55.000000   \n",
       "unique                        NaN           NaN                    NaN   \n",
       "top                           NaN           NaN                    NaN   \n",
       "freq                          NaN           NaN                    NaN   \n",
       "mean                     0.109461      0.861111               0.589091   \n",
       "std                      0.068414      0.259272               0.330921   \n",
       "min                      0.031590      0.100000               0.100000   \n",
       "25%                      0.056286      0.775000               0.300000   \n",
       "50%                      0.090218      1.000000               0.700000   \n",
       "75%                      0.140353      1.000000               1.000000   \n",
       "max                      0.278385      1.000000               1.000000   \n",
       "\n",
       "                                                    error  execution_time  \n",
       "count                                                  31       86.000000  \n",
       "unique                                                 31             NaN  \n",
       "top     This model's maximum context length is 16385 t...             NaN  \n",
       "freq                                                    1             NaN  \n",
       "mean                                                  NaN        7.540131  \n",
       "std                                                   NaN        3.710658  \n",
       "min                                                   NaN        2.353036  \n",
       "25%                                                   NaN        5.116402  \n",
       "50%                                                   NaN        6.406112  \n",
       "75%                                                   NaN        8.658157  \n",
       "max                                                   NaN       21.762401  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_doc_test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b27dd0-f0df-4551-a972-1a6c0df5ffb9",
   "metadata": {},
   "source": [
    "## HyDE\n",
    "\n",
    "HyDE (Hypothetical document embeddings) refers to the technique of using an LLM\n",
    "to generate example queries that my be used to retrieve a doc. By doing so, the resulting embeddings are automatically \"more aligned\" with the embeddings generated from the query. This comes with an additional indexing cost, since each document requires an additoinal call to an LLM while indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c92d2c2-f410-43cc-9c9f-abc22ef48353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever_factory = langchain_docs.retriever_factories[\"hyde\"]\n",
    "\n",
    "retriever = retriever_factory(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179cf29-2d75-4a04-bbb5-b8f22028fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, retriever),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a04f21-0308-4b00-a6f1-694d98ba7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_test_run.get_aggregate_feedback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8af309-d0c4-4562-a5f0-30ca9f9fd861",
   "metadata": {},
   "source": [
    "# Comparing Embeddings\n",
    "\n",
    "We've been using off-the-shelf GTE-Base embeddings so far to retrieve the docs, but\n",
    "what happens if we switch this up? Let's compare to OpenAI's embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e0b2395-c07e-4eae-bb21-afdda3961cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14a5edab-9a3a-4864-b69f-69bc1c9e7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_retriever = langchain_docs.retriever_factories[\"basic\"](openai_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6757c411-aaa5-42ad-824c-7c0b5b942e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-ample-bag-78' at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/projects/p/81c75e74-c54c-4c5e-9a5d-e841c743ad6d?eval=true\n",
      "\n",
      "View all tests for Dataset LangChain Docs Q&A at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/1e4bf58b-1a61-44fb-bb84-4c5c0e2b4b5b\n",
      "[--------->                                        ] 18/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 86/86\n",
      " Eval quantiles:\n",
      "        embedding_cosine_distance  faithfulness  score_string:accuracy error  \\\n",
      "count                   86.000000     80.000000              86.000000     0   \n",
      "unique                        NaN           NaN                    NaN     0   \n",
      "top                           NaN           NaN                    NaN   NaN   \n",
      "freq                          NaN           NaN                    NaN   NaN   \n",
      "mean                     0.160311      0.480000               0.366279   NaN   \n",
      "std                      0.064935      0.253782               0.280172   NaN   \n",
      "min                      0.062004      0.100000               0.100000   NaN   \n",
      "25%                      0.112516      0.300000               0.100000   NaN   \n",
      "50%                      0.145869      0.500000               0.300000   NaN   \n",
      "75%                      0.193581      0.550000               0.500000   NaN   \n",
      "max                      0.323163      1.000000               1.000000   NaN   \n",
      "\n",
      "        execution_time  \n",
      "count        86.000000  \n",
      "unique             NaN  \n",
      "top                NaN  \n",
      "freq               NaN  \n",
      "mean          9.532229  \n",
      "std          10.833398  \n",
      "min           2.809281  \n",
      "25%           5.453107  \n",
      "50%           7.180150  \n",
      "75%           9.476458  \n",
      "max          70.177962  \n"
     ]
    }
   ],
   "source": [
    "openai_embeddings_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, openai_retriever),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae7cbe-a8eb-4b40-aeae-f9c7f4bf335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings_test_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef164b9-7124-4907-b2b4-0595bf3b3441",
   "metadata": {},
   "source": [
    "## Comparing Models\n",
    "\n",
    "We used OpenAI's gpt-3.5-turbo in our previous tests, but lets try with some other models.\n",
    "\n",
    "You can swap in any LangChain LLM within the response generator below.\n",
    "We'll try anthropic's claude-2 first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c86c7-9754-4527-a1a9-a89beba437b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOllama\n",
    "\n",
    "# # A llama2-based model with 128k context\n",
    "# # (in theory) In practice, we will see how well\n",
    "# # it actually leverages that context.\n",
    "# ollama = ChatOllama(model=\"yarn-llama2:7b-128k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82dfe58e-17ed-4563-9db6-2c688c0bf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "\n",
    "# A llama2-based model with 128k context\n",
    "# (in theory) In practice, we will see how well\n",
    "# it actually leverages that context.\n",
    "# fireworks = ChatFireworks(model=\"yarn-llama2:7b-128k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc7dff86-2b93-490a-81ab-72e757e8f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll go back to the GTE embeddings for now\n",
    "\n",
    "retriever_factory = langchain_docs.retriever_factories[\"basic\"]\n",
    "# Since claude-2 has a longer context window, we can fetch more documents by default\n",
    "# Since this only impacts the retriever and NOT the index, this call will load from the cache\n",
    "retriever = retriever_factory(embeddings, search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "94b7dac0-cb4e-40c3-98ec-7720776999b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(model=\"claude-2\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eaa47085-e383-4cc5-9018-5491700c6f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-drab-street-64' at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/projects/p/9c4c3af8-0877-403b-a255-d6acb7279110?eval=true\n",
      "\n",
      "View all tests for Dataset LangChain Docs Q&A at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/1e4bf58b-1a61-44fb-bb84-4c5c0e2b4b5b\n",
      "[>                                                 ] 0/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------>       ] 74/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 963ab58f-bb81-4121-8da4-c5074a5d900a with inputs {'question': 'What is the purpose of caching embeddings?'}\n",
      "Error Type: InternalServerError, Message: Error code: 500 - {'error': {'type': 'api_error', 'message': 'Internal server error'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------> ] 84/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------> ] 85/86"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 86/86\n",
      " Eval quantiles:\n",
      "        embedding_cosine_distance  faithfulness  score_string:accuracy  \\\n",
      "count                   85.000000     79.000000              85.000000   \n",
      "unique                        NaN           NaN                    NaN   \n",
      "top                           NaN           NaN                    NaN   \n",
      "freq                          NaN           NaN                    NaN   \n",
      "mean                     0.173510      0.532911               0.338824   \n",
      "std                      0.061614      0.342959               0.264617   \n",
      "min                      0.068957      0.100000               0.100000   \n",
      "25%                      0.123281      0.300000               0.100000   \n",
      "50%                      0.169280      0.500000               0.300000   \n",
      "75%                      0.208775      1.000000               0.500000   \n",
      "max                      0.322887      1.000000               1.000000   \n",
      "\n",
      "                                                    error  execution_time  \n",
      "count                                                   1       86.000000  \n",
      "unique                                                  1             NaN  \n",
      "top     Error code: 500 - {'error': {'type': 'api_erro...             NaN  \n",
      "freq                                                    1             NaN  \n",
      "mean                                                  NaN       14.181848  \n",
      "std                                                   NaN       31.837816  \n",
      "min                                                   NaN        3.925020  \n",
      "25%                                                   NaN        7.076074  \n",
      "50%                                                   NaN        9.553143  \n",
      "75%                                                   NaN       11.537720  \n",
      "max                                                   NaN      218.654918  \n"
     ]
    }
   ],
   "source": [
    "claude_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(chain_factory, llm=llm, retriever=retriever),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98edcf42-a405-400b-882e-04de2559359c",
   "metadata": {},
   "source": [
    "## Changing the prompt in the response generator\n",
    "\n",
    "The default prompt was tested primariily on OpenAI's gpt-3.5 model. When switching models, you may get better results if you modify the prompt. Let's try a simple one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "69d3b36f-68aa-4005-9bb2-de228491ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "64caac6f-888d-432c-9329-5c4b97ad859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"wfh/rag-simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c0e6762-1e50-4eef-833a-a4a2bf8883ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = prompt | ChatAnthropic(model=\"claude-2\", temperature=1) | StrOutputParser()\n",
    "new_chain = chain_factory(response_synthesizer=generator, retriever=openai_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96886de0-a653-4875-a68f-5a11efcb200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'test-long-seat-30' at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/projects/p/c03ed2a9-a1c8-493e-b42d-66e5a1ce6567?eval=true\n",
      "\n",
      "View all tests for Dataset LangChain Docs Q&A at:\n",
      "http://localhost/o/00000000-0000-0000-0000-000000000000/datasets/1e4bf58b-1a61-44fb-bb84-4c5c0e2b4b5b\n",
      "[>                                                 ] 0/86"
     ]
    }
   ],
   "source": [
    "claude_simple_prompt_test_run = client.run_on_dataset(\n",
    "    dataset_name=langchain_docs.name,\n",
    "    llm_or_chain_factory=partial(\n",
    "        chain_factory, response_synthesizer=generator, retriever=retriever\n",
    "    ),\n",
    "    evaluation=RAG_EVALUATION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe4ce6-e872-4d28-8ac4-a18ed28a8af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
